\section{Services for Data Quality Analysis (SDQA)}
\label{sec:sdqa}

\subsection{Key Requirements}

SQDA is a set of loosely coupled services intended to service LSST's quality assessement needs through all phases of Construction, Commissioning and Operations. Consumers of these services may include developers, facility staff, DAC (eg. L3) users, and the general LSST science user community. Use of these services is intended for routine characterisation, fault detection and fault diagnosis.

\begin{itemize}
\item SDQA shall provide sevices for science data quality analysis of Level 1, 2, and Calibration Processing pipelines.

\item SDQA shall provide services to support software development in Construction, Commissioning and Operations.

\item SDQA shall provide for the visualization, analysis and monitoring capabilities needed for common science quality data analysis usecases. Its inputs may be gathered from SDQA itself, the production pipelines, engineering data sources and non-LSST data sources.

\item SDQA shall have the flexibility to support execution of ad-hoc (user-driven) tests and analyses of ad-hoc data within a standard framework.

\item SDQA shall support usecases involving interactive ``drill-down'' of QA data exposed through its visualisation interfaces.

\item SDQA shall allow for notifications to be issued when monitoring quantities that exceed their permissible thresholds and/or have degraded over historical values.

\item SDQA shall be able to collect and harvest the outputs and logs of execution of the production pipelines, and extract and expose metrics from these logs.

\item SQDA shall make provision to store outputs that are not stored through other LSST data access services.

\item SDQA should be deployable as high-reliability scalable services for production as well as allow for core data assessment functionality to be executed on a developer's local machine.

\item SDQA shall be architected in a manner that would enable it to be deployable on standard cloud architectures outside of the LSST facilities.


\end{itemize}


\subsection{Key Tasks for Each Level of QA}

SDQA system will provide a framework that is capable of monitoring QA
information at four different stages of capabiliti and maturity:

\begin{itemize}
\item QA Level 0 - Testing and Validation of the DM sub-system in pre-commissioning
\item QA Level 1 - Real-time data quality and system assesment during commissioning + operations (also, forensics)
\item QA Level 2 - Quality assessment of Data Releases (also, forensics)
\item QA Level 3 - Ability for the community to evaluate the data quality of their own analyses. These should available as well-documented and reployable versions of core QA Level 0--2 services.
\end{itemize}

\subsubsection{QA0}

Test the DM software during pre-commissioning as well as test software improvements during commissioning and operations, quantifying the software performance against known and/or expected outputs.

Validating the software and its performance on stamdardized data.

(“Make me a three-color diagram, compute the width of point sources in the blue part of the locus”)

(“I have 20 visits all over the sky, I want to match up the results”)

The main components:
\begin{enumerate}
\item CI system that compiles code
\item Test execution harness – that runs test on a regular cadence eg nightly/weekly/monthly
\item A suite of validation metrics codes – some has to come from Science Pipelines, but KPMs are delivered by SQuaRE
\item Capability to instrument the production pipelines for computational performance metrics
\item Library of “instrumentations”
\item Interface to data products and QA metrics (including visualization)
    \begin{enumerate}
    \item Tabular query result interface
    \item Visualizer for images
    \item Visualiser for scalar data (eg. Plotter)
    \item SuperTask execution on selected data
    \end{enumerate}
\item Curated datasets to use in tests
\item Capability for interactive analysis of QA outputs (drilldown into existing tests, ad hoc tests, ad hoc afterburners) – some has to come from Science Pipelines or SUIT but SQuaRE provides examples [move to Shared Software Components section]
    \begin{enumerate}
    \item Tools that perform computations
    \item Tools that perform visualization (using Butler if astronomical, maybe direct database if not)
    \end{enumerate}
\item Connection from analysis toolkit to validation metrics (attach common interactive plots to validation metrics)
\item QA database including ingestion
\item Notification system for threshold crossings
\end{enumerate}

Prototypes for all of these exist except ``Toolkit for analysis of QA outputs'' and ``Connection from analysis toolkit to validation metrics''

``Toolkit for analysis of QA outputs'' will take more resources than the others listed above, but some may be already scheduled in other teams

\subsubsection{QA1}
Data quality in real time during commissioning and operations. Analyzes the data stream in near-real time, information about observing conditions (sky brightness, transparency, seeing, magnitude limit) as well as characterize subtle deterioration in system performance.

Validating the operational system.

Main components reused from Level QA0:
\begin{enumerate}
\item Library of validation metrics codes
\item Instrumentation capability for computational performance metrics
\item Library of “instrumentations”
\item Interface to results (including visualization)
\item Curated datasets to use in tests
\item Capability for interactive analysis of failures (drilldown into existing tests, ad hoc tests, ad hoc afterburners) – some has to come from Science Pipelines or SUIT but SQuaRE provides examples
\item Connection from analysis toolkit to validation metrics
\item QA data access service (including ingesting it as well as querying it)
\end{enumerate}

Main components reused from Level QA0: All.

Additional components required for Level QA1 services:

\begin{enumerate}
\item Harness for analyzing alert contents (and perhaps format)
\item Faster metrics codes to meet overall 60 second performance requirement for alert publication (but not necessarily for all QA processing, which must meet only throughput requirements)
\item Additional metrics/instrumentation codes (that must not disturb production system, including its performance, when dynamically inserted)
\item Output interface to “comfort” display (aggregation, trending, etc.)
\item Output interface to automated systems (drop alerts, reschedule field, etc.)
\item Correlator between telemetry streams and metrics
\item Input interface from sources of data not already present in Prompt Processing system
\item Fake source injection and analysis
\item Metrics codes specific for calibration/engineering/special- purpose images
\end{enumerate}

\subsubsection{QA2}
Assess the quality of data releases (including the co-added image data products) performing quality assessment for astrometric and photometric calibration and derived products, looking for problems with the image processing pipelines and systematic problems with the instrument.
Validating the Data Release products.
All components from QA0

New main components:
\begin{enumerate}
\item DRP-specific dataset
\item Release data product editing tools (including provenance tracking)
\item Output interface to workflow system based on QA results and provenance
\item Provenance analysis tools
\item Output interface to Science Pipelines, including from QA database
\item Comparison tools for overlap areas due to satellite processing
\item Metrics/products for science users to understand quality of science data products (depth mask/selection function, etc.)
\item Characterization report for Data Release
\end{enumerate}

\subsubsection{QA3}
Data quality based on science analysis performed by the LSST Science Collaborations and the community. Level 0-2 visualization and data exploration tools will be made available to the community.
Make all results from the above available. Make all of the above components available to some part of the community (could be just affiliated data centers or could be individual scientists) as a supported product.
Ingest external science analysis data as Level 3 data products; ingest useful external science analysis tools.

\subsubsection{Prototype Implementation of PipeQA}

The pipeQA prototype is a useful reference for exploring ideas and we mention
it here to capture this prototype work.

A prototype implementation of the SDQA was implemented in LSST Final Design Phase. The existing prototype was tested with image simulation inputs, as well as real data (SDSS Stripe 82).
\\

The prototype used a set of statically and dynamically generated pages (written in php) to display the results of data production runs. While proving invaluable for data analysis, the prototype design was found it to be difficult to extend with new analyst-developed tests.
\\

The prototype code is available in the \url{https://github.com/lsst/testing_displayQA} git repository.
